{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Proposal for Applied Machine Learning (i526)  \n",
    "\n",
    "---\n",
    "### Can Machine Learning Beat the Vegas Spread for NFL Games?\n",
    "#### By: Shivam Kapadia & Peter Russell  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Motivation\n",
    "\n",
    "For Las Vegas, sports wagering is a lucrative business. In 2017, Nevada sportsbooks took in over $ \\$ $ 4.8 billion of wagers and won a record $ \\$ $ 248.7 million. This cleared the previous record by $17 million [1]. These profits did not come from taking the money of unlucky or unskilled gamblers, but instead comes largely from pairing up opposing bets and collecting the spread between the two offsetting bets. As a result, it is in the sportsbook best interest to find the most fair line that generates the largest volume of bets on both sides. Few are more adept at determining a fair line for a sports game than Vegas sportsbook for this very reason. As a testament to their skill, over the last 15 regular seasons, NFL favorites have gone 1,859-1,860 against the spread, with 111 pushes, which is an amazingly symmetrical distribution [2]. \n",
    "\n",
    "The NFL is the most wagered sport in the country, responsible for 36\\% of all bets in Nevada. Baseball is the next closest at 23\\% [3]. The United States is currently undergoing to a dramatic transformation as it relates to sports betting. The Supreme Court has decided that states outside of Nevada will be allowed to offer sports betting, which is a windfall that is expected to add $\\$ $2.3 billion to the NFL and will make the sportsbook landscape even more competitive [4]. With this in mind, we are interested in applying the developments in machine learning and techniques learned throughout the semester to see if we can build a sports betting model that will not only beat the baseline success rate of 50\\%, but also the spread between opposing wagers' payouts of roughly 10 \\%. Namely, we hope to build a model that will have a success rate of 60 \\% for pure classification predictions of \"cover the spread\" or \"not cover the spread.\" Additionally, as a stretch goal, we would like to weight our predictions by our confidence level in the predictions for optimal money wagering management. \n",
    "\n",
    "[1] http://www.espn.com/chalk/story/_/id/22273982/record-amounts-money-bet-lost-nevada-2017\n",
    "\n",
    "[2] https://abcnews.go.com/Sports/betting-nfl-season/story?id=57617459\n",
    "\n",
    "[3] https://www.sportsbusinessdaily.com/Journal/Issues/2018/04/16/World-Congress-of-Sports/Research.aspx\n",
    "\n",
    "[4] https://www.legalsportsreport.com/23596/nfl-sports-betting-revenue-survey/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "**Source:** Kaggle (public)\n",
    "https://www.kaggle.com/tobycrabtree/nfl-scores-and-betting-data\n",
    "\n",
    "**NFL Stadium Information (100 x 15):** Name, Location, Open/Close, Type, Address, Weather Station Code, Weather Type, Capacity, Surface, Longitude, Latitude\n",
    "\n",
    "**NFL Teams (41 x 8):** Name (Short and Long), Conference, Division, Conference (pre-2002), Division (pre-2002)\n",
    "\n",
    "**Scores (12.4k x 17):** Date, Season, Week, Team Home, Team Away, Stadium, Favorite, Spread (Favorite), Over/Under Line, Weather, Wind, Humidity, Score, Stadium, Playoff/Regular Season\n",
    "\n",
    "Our data is largely limited to the conditions of the game being played versus the characteristics of the team. We will try to use the dataset to infer features that incorporate the momentum of a team (eg. points scored over the last 3 games, points given up) to see if they hold importance in the model. In some ways though, having previous season data would not be expected be tremendously helpful with the exception of a few teams (eg. Patriots) as roster turnover is large in the NFL and team strength can vary significantly year-to-year. Having week-by-week statistics ('Points Scored per Game','Opponent Points Scored per Game','Giveaways per Game','Takeaways per Game','Yards per Game','Opponent Yards per Game','Penalty Yards per Game') for a team would be help, but the data is difficult to obtain. We have put a request out to a provider of the data and are waiting to hear back (https://www.teamrankings.com/nfl). This would add potentially 6 new features with each having approximately 255 observations (15 years of data available for 17 weeks each). \n",
    "\n",
    "We have found aggregated season data if we choose to incorporate this, perhaps as a dummy variable if the team was a top 4 finisher at the end of the previous season (https://www.kaggle.com/farmander/nfl-statistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Choice\n",
    "\n",
    "To begin, we will be working with binary classifiers to fit our model where the binary outcomes will be 'covered spread' and 'did not cover spread'. We will be interested in the point spread and the over/under totals for measuring against the spread. \n",
    "\n",
    "The models that we will be exploring will be Random Forest Classifier to help the model decide which data elements have critical break points that will help us determine if the point spread will be covered. Additionally, we will look at Logistic Regression, KNN and SVM as our other binary classifiers as benchmarks. \n",
    "\n",
    "Time permitting, we will look at Linear Regression model to predict the point spread and use this difference from the actual point spread to weight the size of our bet. As of now, this is a stretch goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are concerned with whether or not our model can successfully predict the correct spread outcome, we will be using accuracy as our scoring mechanism in the binary classifier. \n",
    "\n",
    "If we are able to reach our stretch goal, we will use mean square error as the metric we wish to minimize for the most accurate spread prediction that we can then compare against the actual spread level for our wager weighting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gantt Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T04:45:51.384847Z",
     "start_time": "2018-11-13T04:45:34.574190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~petrusse/14.embed\" height=\"600px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='petrusse', api_key='jUnAolU7CUgK34fHqUDw')\n",
    "\n",
    "df = [dict(Task=\"Perfom EDA\", Start='2018-11-11', Finish='2018-11-12', Resource='Phase1'),\n",
    "      dict(Task=\"Form Pipeline\", Start='2018-11-12', Finish='2018-11-13', Resource='Phase1'),\n",
    "      dict(Task=\"Train Data\", Start='2018-11-13', Finish='2018-11-15', Resource='Phase1'),\n",
    "      dict(Task=\"Form Results Table\", Start='2018-11-13', Finish='2018-11-15', Resource='Phase1'),\n",
    "      dict(Task=\"Review Results\", Start='2018-11-15', Finish='2018-11-17', Resource='Phase1'),\n",
    "      dict(Task=\"Slides / Video\", Start='2018-11-16', Finish='2018-11-17', Resource='Phase1'),\n",
    "      dict(Task=\"Modify Pipeline\", Start='2018-11-17', Finish='2018-11-20', Resource='Phase2'),\n",
    "      dict(Task=\"Re-train Data\", Start='2018-11-20', Finish='2018-11-25', Resource='Phase2'),\n",
    "      dict(Task=\"Hyperparameters\", Start='2018-11-25', Finish='2018-12-01', Resource='Phase2'),\n",
    "      dict(Task=\"Slides / Video\", Start='2018-11-30', Finish='2018-12-01', Resource='Phase2'),\n",
    "      dict(Task=\"Hyperparameters\", Start='2018-12-01', Finish='2018-12-06', Resource='Final'),\n",
    "      dict(Task=\"Presentation\", Start='2018-12-06', Finish='2018-12-12', Resource='Final')\n",
    "     ]\n",
    "\n",
    "colors = dict(Phase1 = 'rgb(220, 0, 0)',\n",
    "              Phase2 = 'rgb(170, 14, 200)',\n",
    "              Final = (1, 0.9, 0.16))\n",
    "\n",
    "fig = ff.create_gantt(df, colors=colors, index_col='Resource', show_colorbar=True)\n",
    "py.iplot(fig, filename='gantt-dictionary-colors', world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T04:45:51.424976Z",
     "start_time": "2018-11-13T04:45:51.391922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"gantt-dictionary-colors.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"gantt-dictionary-colors.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline will require the integration of both numerical data (team statistics, spreads) along with categorical data (type of stadium, weather conditions). Within the pipeline, we will standardize the team statistics across the entire dataset. We will ultimately lean on this pipeline in our grid search in tuning the hyperparameters to find the best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team Responsibilities\n",
    "\n",
    "**Pete**: Complete the EDA with tables and visuals. Optimize Logistic Regression Model.\n",
    "\n",
    "**Shivam**: Formulate the Pipeline that incorporates the numerical and categorical data. Optimize Random Forest Classifier Model. \n",
    "\n",
    "**Together**: Review results, create slides and video. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
